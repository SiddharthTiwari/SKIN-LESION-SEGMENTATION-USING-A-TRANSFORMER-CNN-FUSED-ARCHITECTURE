{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270223d1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6683e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "import configparser\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torchvision.models import resnet34 as resnet\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "\n",
    "from functools import partial\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.helpers import load_pretrained\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9eea6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f6484",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18962da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = 'data/' # change to your data folder path\n",
    "# data_f = ['ISIC-2017_Training_Data/', 'ISIC-2017_Validation_Data/', 'ISIC-2017_Test_v2_Data/']\n",
    "# mask_f = ['ISIC-2017_Training_Part1_GroundTruth/', 'ISIC-2017_Validation_Part1_GroundTruth/', 'ISIC-2017_Test_v2_Part1_GroundTruth/']\n",
    "# set_size = [2000, 150, 600]\n",
    "# save_name = ['train', 'val', 'test']\n",
    "\n",
    "# height = 192\n",
    "# width = 256\n",
    "\n",
    "# for j in range(3):\n",
    "\n",
    "#     print('processing ' + data_f[j] + '......')\n",
    "#     count = 0\n",
    "#     length = set_size[j]\n",
    "#     imgs = np.uint8(np.zeros([length, height, width, 3]))\n",
    "#     masks = np.uint8(np.zeros([length, height, width]))\n",
    "\n",
    "#     path = root + data_f[j]\n",
    "#     mask_p = root + mask_f[j]\n",
    "\n",
    "#     for i in os.listdir(path):\n",
    "#         if len(i.split('_'))==2:\n",
    "#             img = cv2.imread(path+i)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#             img = cv2.resize(img, (width, height))\n",
    "\n",
    "#             m_path = mask_p + i.replace('.jpg', '_segmentation.png')\n",
    "#             mask = cv2.imread(m_path, 0)\n",
    "#             mask = cv2.resize(mask, (width, height))\n",
    "\n",
    "#             imgs[count] = img\n",
    "#             masks[count] = mask\n",
    "\n",
    "#             count +=1 \n",
    "#             if count%100==0:\n",
    "#                 print(count)\n",
    "\n",
    "\n",
    "#     np.save('{}/data_{}.npy'.format(root, save_name[j]), imgs)\n",
    "#     np.save('{}/mask_{}.npy'.format(root, save_name[j]), masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a725b",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567dfdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # FIXME look at relaxing size constraints\n",
    "        #assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #    f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
    "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
    "        #self.repr_act = nn.Tanh()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _conv_filter(state_dict, patch_size=16):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k:\n",
    "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_small_patch16_224(pretrained=False, **kwargs):\n",
    "    if pretrained:\n",
    "        # NOTE my scale was wrong for original weights, leaving this here until I have better ones for this model\n",
    "        kwargs.setdefault('qk_scale', 768 ** -0.5)\n",
    "    model = VisionTransformer(patch_size=16, embed_dim=768, depth=8, num_heads=8, mlp_ratio=3., **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_small_patch16_224']\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch16_224(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_base_patch16_224']\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch16_384(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_base_patch16_384']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch32_384(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=384, patch_size=32, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_base_patch32_384']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_large_patch16_224(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_large_patch16_224']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_large_patch16_384(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_large_patch16_384']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_large_patch32_384(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=384, patch_size=32, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_large_patch32_384']\n",
    "    if pretrained:\n",
    "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_huge_patch16_224(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_huge_patch16_224']\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_huge_patch32_384(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=384, patch_size=32, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n",
    "    model.default_cfg = default_cfgs['vit_huge_patch32_384']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3281fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
    "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
    "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
    "    'deit_base_distilled_patch16_384',\n",
    "]\n",
    "\n",
    "\n",
    "class DeiT(VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, self.embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "        # with slight modifications to add the dist_token\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        pe = self.pos_embed\n",
    "\n",
    "        x = x + pe\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
    "    model = DeiT(\n",
    "        patch_size=16, embed_dim=384, depth=8, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    model.default_cfg = _cfg()\n",
    "    if pretrained:\n",
    "        ckpt = torch.load('pretrained/deit_small_patch16_224-cd65a155.pth')\n",
    "        model.load_state_dict(ckpt['model'], strict=False)\n",
    "    \n",
    "    pe = model.pos_embed[:, 1:, :].detach()\n",
    "    pe = pe.transpose(-1, -2)\n",
    "    pe = pe.view(pe.shape[0], pe.shape[1], int(np.sqrt(pe.shape[2])), int(np.sqrt(pe.shape[2])))\n",
    "    pe = F.interpolate(pe, size=(12, 16), mode='bilinear', align_corners=True)\n",
    "    pe = pe.flatten(2)\n",
    "    pe = pe.transpose(-1, -2)\n",
    "    model.pos_embed = nn.Parameter(pe)\n",
    "    model.head = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40288c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit = deit_small_patch16_224\n",
    "\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1)\n",
    "\n",
    "\n",
    "class BiFusion_block(nn.Module):\n",
    "    def __init__(self, ch_1, ch_2, r_2, ch_int, ch_out, drop_rate=0.):\n",
    "        super(BiFusion_block, self).__init__()\n",
    "\n",
    "        # channel attention for F_g, use SE Block\n",
    "        self.fc1 = nn.Conv2d(ch_2, ch_2 // r_2, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(ch_2 // r_2, ch_2, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # spatial attention for F_l\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = Conv(2, 1, 7, bn=True, relu=False, bias=False)\n",
    "\n",
    "        # bi-linear modelling for both\n",
    "        self.W_g = Conv(ch_1, ch_int, 1, bn=True, relu=False)\n",
    "        self.W_x = Conv(ch_2, ch_int, 1, bn=True, relu=False)\n",
    "        self.W = Conv(ch_int, ch_int, 3, bn=True, relu=True)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.residual = Residual(ch_1+ch_2+ch_int, ch_out)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(drop_rate)\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        # bilinear pooling\n",
    "        W_g = self.W_g(g)\n",
    "        W_x = self.W_x(x)\n",
    "        bp = self.W(W_g*W_x)\n",
    "\n",
    "        # spatial attention for cnn branch\n",
    "        g_in = g\n",
    "        g = self.compress(g)\n",
    "        g = self.spatial(g)\n",
    "        g = self.sigmoid(g) * g_in\n",
    "\n",
    "        # channel attetion for transformer branch\n",
    "        x_in = x\n",
    "        x = x.mean((2, 3), keepdim=True)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x) * x_in\n",
    "        fuse = self.residual(torch.cat([g, x, bp], 1))\n",
    "\n",
    "        if self.drop_rate > 0:\n",
    "            return self.dropout(fuse)\n",
    "        else:\n",
    "            return fuse\n",
    "\n",
    "\n",
    "class TransFuse_S(nn.Module):\n",
    "    def __init__(self, num_classes=1, drop_rate=0.2, normal_init=True, pretrained=False):\n",
    "        super(TransFuse_S, self).__init__()\n",
    "\n",
    "        self.resnet = resnet()\n",
    "        if pretrained:\n",
    "            self.resnet.load_state_dict(torch.load('pretrained/resnet34-43635321.pth'))\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.resnet.layer4 = nn.Identity()\n",
    "\n",
    "        self.transformer = deit(pretrained=pretrained)\n",
    "\n",
    "        self.up1 = Up(in_ch1=384, out_ch=128)\n",
    "        self.up2 = Up(128, 64)\n",
    "\n",
    "        self.final_x = nn.Sequential(\n",
    "            Conv(256, 64, 1, bn=True, relu=True),\n",
    "            Conv(64, 64, 3, bn=True, relu=True),\n",
    "            Conv(64, num_classes, 3, bn=False, relu=False)\n",
    "            )\n",
    "\n",
    "        self.final_1 = nn.Sequential(\n",
    "            Conv(64, 64, 3, bn=True, relu=True),\n",
    "            Conv(64, num_classes, 3, bn=False, relu=False)\n",
    "            )\n",
    "\n",
    "        self.final_2 = nn.Sequential(\n",
    "            Conv(64, 64, 3, bn=True, relu=True),\n",
    "            Conv(64, num_classes, 3, bn=False, relu=False)\n",
    "            )\n",
    "\n",
    "        self.up_c = BiFusion_block(ch_1=256, ch_2=384, r_2=4, ch_int=256, ch_out=256, drop_rate=drop_rate/2)\n",
    "\n",
    "        self.up_c_1_1 = BiFusion_block(ch_1=128, ch_2=128, r_2=2, ch_int=128, ch_out=128, drop_rate=drop_rate/2)\n",
    "        self.up_c_1_2 = Up(in_ch1=256, out_ch=128, in_ch2=128, attn=True)\n",
    "\n",
    "        self.up_c_2_1 = BiFusion_block(ch_1=64, ch_2=64, r_2=1, ch_int=64, ch_out=64, drop_rate=drop_rate/2)\n",
    "        self.up_c_2_2 = Up(128, 64, 64, attn=True)\n",
    "\n",
    "        self.drop = nn.Dropout2d(drop_rate)\n",
    "\n",
    "        if normal_init:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, imgs, labels=None):\n",
    "        # bottom-up path\n",
    "        x_b = self.transformer(imgs)\n",
    "        x_b = torch.transpose(x_b, 1, 2)\n",
    "        x_b = x_b.view(x_b.shape[0], -1, 12, 16)\n",
    "        x_b = self.drop(x_b)\n",
    "\n",
    "        x_b_1 = self.up1(x_b)\n",
    "        x_b_1 = self.drop(x_b_1)\n",
    "\n",
    "        x_b_2 = self.up2(x_b_1)  # transformer pred supervise here\n",
    "        x_b_2 = self.drop(x_b_2)\n",
    "\n",
    "        # top-down path\n",
    "        x_u = self.resnet.conv1(imgs)\n",
    "        x_u = self.resnet.bn1(x_u)\n",
    "        x_u = self.resnet.relu(x_u)\n",
    "        x_u = self.resnet.maxpool(x_u)\n",
    "\n",
    "        x_u_2 = self.resnet.layer1(x_u)\n",
    "        x_u_2 = self.drop(x_u_2)\n",
    "\n",
    "        x_u_1 = self.resnet.layer2(x_u_2)\n",
    "        x_u_1 = self.drop(x_u_1)\n",
    "\n",
    "        x_u = self.resnet.layer3(x_u_1)\n",
    "        x_u = self.drop(x_u) \n",
    "\n",
    "        # joint path\n",
    "        x_c = self.up_c(x_u, x_b)\n",
    "\n",
    "        x_c_1_1 = self.up_c_1_1(x_u_1, x_b_1)\n",
    "        x_c_1 = self.up_c_1_2(x_c, x_c_1_1)\n",
    "\n",
    "        x_c_2_1 = self.up_c_2_1(x_u_2, x_b_2)\n",
    "        x_c_2 = self.up_c_2_2(x_c_1, x_c_2_1) # joint predict low supervise here\n",
    "\n",
    "        # decoder part\n",
    "        map_x = F.interpolate(self.final_x(x_c), scale_factor=16, mode='bilinear')\n",
    "        map_1 = F.interpolate(self.final_1(x_b_2), scale_factor=4, mode='bilinear')\n",
    "        map_2 = F.interpolate(self.final_2(x_c_2), scale_factor=4, mode='bilinear')\n",
    "        return map_x, map_1, map_2\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.up1.apply(init_weights)\n",
    "        self.up2.apply(init_weights)\n",
    "        self.final_x.apply(init_weights)\n",
    "        self.final_1.apply(init_weights)\n",
    "        self.final_2.apply(init_weights)\n",
    "        self.up_c.apply(init_weights)\n",
    "        self.up_c_1_1.apply(init_weights)\n",
    "        self.up_c_1_2.apply(init_weights)\n",
    "        self.up_c_2_1.apply(init_weights)\n",
    "        self.up_c_2_2.apply(init_weights)\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "    Initialize weights of layers using Kaiming Normal (He et al.) as argument of \"Apply\" function of\n",
    "    \"nn.Module\"\n",
    "    :param m: Layer to initialize\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        '''\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "        trunc_normal_(m.weight, std=math.sqrt(1.0/fan_in)/.87962566103423978)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "        '''\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(m.bias, -bound, bound)\n",
    "        \n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_ch1, out_ch, in_ch2=0, attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_ch1+in_ch2, out_ch)\n",
    "\n",
    "        if attn:\n",
    "            self.attn_block = Attention_block(in_ch1, in_ch2, out_ch)\n",
    "        else:\n",
    "            self.attn_block = None\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        if x2 is not None:\n",
    "            diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "            diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                            diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "            if self.attn_block is not None:\n",
    "                x2 = self.attn_block(x1, x2)\n",
    "            x1 = torch.cat([x2, x1], dim=1)\n",
    "        x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.identity = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.double_conv(x)+self.identity(x))\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        super(Residual, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn1 = nn.BatchNorm2d(inp_dim)\n",
    "        self.conv1 = Conv(inp_dim, int(out_dim/2), 1, relu=False)\n",
    "        self.bn2 = nn.BatchNorm2d(int(out_dim/2))\n",
    "        self.conv2 = Conv(int(out_dim/2), int(out_dim/2), 3, relu=False)\n",
    "        self.bn3 = nn.BatchNorm2d(int(out_dim/2))\n",
    "        self.conv3 = Conv(int(out_dim/2), out_dim, 1, relu=False)\n",
    "        self.skip_layer = Conv(inp_dim, out_dim, 1, relu=False)\n",
    "        if inp_dim == out_dim:\n",
    "            self.need_skip = False\n",
    "        else:\n",
    "            self.need_skip = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.need_skip:\n",
    "            residual = self.skip_layer(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out += residual\n",
    "        return out \n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, kernel_size=3, stride=1, bn=False, relu=True, bias=True):\n",
    "        super(Conv, self).__init__()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, kernel_size, stride, padding=(kernel_size-1)//2, bias=bias)\n",
    "        self.relu = None\n",
    "        self.bn = None\n",
    "        if relu:\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size()[1] == self.inp_dim, \"{} {}\".format(x.size()[1], self.inp_dim)\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccb3e5",
   "metadata": {},
   "source": [
    "## Define DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9774cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for skin lesion segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root):\n",
    "        self.images = np.load(image_root)\n",
    "        self.gts = np.load(gt_root)\n",
    "        self.size = len(self.images)\n",
    "\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.gt_transform = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "        \n",
    "        self.transform = A.Compose(\n",
    "            [\n",
    "                A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=25, p=0.5, border_mode=0),\n",
    "                A.ColorJitter(),\n",
    "                A.HorizontalFlip(),\n",
    "                A.VerticalFlip()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.images[index]\n",
    "        gt = self.gts[index]\n",
    "        gt = gt/255.0\n",
    "\n",
    "        transformed = self.transform(image=image, mask=gt)\n",
    "        image = self.img_transform(transformed['image'])\n",
    "        gt = self.gt_transform(transformed['mask'])\n",
    "        return image, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "def get_loader(image_root, gt_root, batchsize, shuffle=True, num_workers=4, pin_memory=True):\n",
    "\n",
    "    dataset = SkinDataset(image_root, gt_root)\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batchsize,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=pin_memory)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class test_dataset:\n",
    "    def __init__(self, image_root, gt_root):\n",
    "        self.images = np.load(image_root)\n",
    "        self.gts = np.load(gt_root)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        self.gt_transform = transforms.ToTensor()\n",
    "        self.size = len(self.images)\n",
    "        self.index = 0\n",
    "\n",
    "    def load_data(self):\n",
    "        image = self.images[self.index]\n",
    "        image = self.transform(image).unsqueeze(0)\n",
    "        gt = self.gts[self.index]\n",
    "        gt = gt/255.0\n",
    "        self.index += 1\n",
    "\n",
    "        return image, gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ae30a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cd530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_np(y_true, y_pred, **kwargs):\n",
    "    \"\"\"\n",
    "    compute mean iou for binary segmentation map via numpy\n",
    "    \"\"\"\n",
    "    axes = (0, 1) \n",
    "    intersection = np.sum(np.abs(y_pred * y_true), axis=axes) \n",
    "    mask_sum = np.sum(np.abs(y_true), axis=axes) + np.sum(np.abs(y_pred), axis=axes)\n",
    "    union = mask_sum  - intersection \n",
    "    \n",
    "    smooth = .001\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def mean_dice_np(y_true, y_pred, **kwargs):\n",
    "    \"\"\"\n",
    "    compute mean dice for binary segmentation map via numpy\n",
    "    \"\"\"\n",
    "    axes = (0, 1) # W,H axes of each image\n",
    "    intersection = np.sum(np.abs(y_pred * y_true), axis=axes) \n",
    "    mask_sum = np.sum(np.abs(y_true), axis=axes) + np.sum(np.abs(y_pred), axis=axes)\n",
    "    \n",
    "    smooth = .001\n",
    "    dice = 2*(intersection + smooth)/(mask_sum + smooth)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11feb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter(object):\n",
    "    def __init__(self, num=40):\n",
    "        self.num = num\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)\n",
    "\n",
    "    def show(self):\n",
    "        return torch.mean(torch.stack(self.losses[np.maximum(len(self.losses)-self.num, 0):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095443a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1)/(union - inter+1)\n",
    "    return (wbce + wiou).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c314c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "train_config=config['DEFAULT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2e422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, best_loss):\n",
    "    model.train()\n",
    "    loss_record2, loss_record3, loss_record4 = AvgMeter(), AvgMeter(), AvgMeter()\n",
    "    accum = 0\n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        # ---- data prepare ----\n",
    "        images, gts = pack\n",
    "        images = Variable(images).cuda()\n",
    "        gts = Variable(gts).cuda()\n",
    "\n",
    "        # ---- forward ----\n",
    "        lateral_map_4, lateral_map_3, lateral_map_2 = model(images)\n",
    "\n",
    "        # ---- loss function ----\n",
    "        loss4 = structure_loss(lateral_map_4, gts)\n",
    "        loss3 = structure_loss(lateral_map_3, gts)\n",
    "        loss2 = structure_loss(lateral_map_2, gts)\n",
    "\n",
    "        loss = 0.5 * loss2 + 0.3 * loss3 + 0.2 * loss4\n",
    "\n",
    "        # ---- backward ----\n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), float(train_config['grad_norm']))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- recording loss ----\n",
    "        loss_record2.update(loss2.data, int(train_config['batchsize']))\n",
    "        loss_record3.update(loss3.data, int(train_config['batchsize']))\n",
    "        loss_record4.update(loss4.data, int(train_config['batchsize']))\n",
    "\n",
    "        # ---- train visualization ----\n",
    "        if i % 20 == 0 or i == total_step:\n",
    "            print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], '\n",
    "                  '[lateral-2: {:.4f}, lateral-3: {:0.4f}, lateral-4: {:0.4f}]'.  \n",
    "                  format(datetime.now(), epoch, int(train_config['epoch']) , i, total_step,\n",
    "                         loss_record2.show(), loss_record3.show(), loss_record4.show()))\n",
    "\n",
    "    save_path = 'snapshots/{}/'.format(train_config['train_save'])\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        meanloss = test(model, train_config['test_path'])\n",
    "        if meanloss < best_loss:\n",
    "            print('new best loss: ', meanloss)\n",
    "            best_loss = meanloss\n",
    "            torch.save(model.state_dict(), save_path + 'TransFuse-%d.pth' % epoch)\n",
    "            print('[Saving Snapshot:]', save_path + 'TransFuse-%d.pth'% epoch)\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def test(model, path):\n",
    "\n",
    "    model.eval()\n",
    "    mean_loss = []\n",
    "\n",
    "    for s in ['val', 'test']:\n",
    "        image_root = '{}/data_{}.npy'.format(path, s)\n",
    "        gt_root = '{}/mask_{}.npy'.format(path, s)\n",
    "        test_loader = test_dataset(image_root, gt_root)\n",
    "\n",
    "        dice_bank = []\n",
    "        iou_bank = []\n",
    "        loss_bank = []\n",
    "        acc_bank = []\n",
    "\n",
    "        for i in range(test_loader.size):\n",
    "            image, gt = test_loader.load_data()\n",
    "            image = image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, res = model(image)\n",
    "            loss = structure_loss(res, torch.tensor(gt).unsqueeze(0).unsqueeze(0).cuda())\n",
    "\n",
    "            res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "            gt = 1*(gt>0.5)            \n",
    "            res = 1*(res > 0.5)\n",
    "\n",
    "            dice = mean_dice_np(gt, res)\n",
    "            iou = mean_iou_np(gt, res)\n",
    "            acc = np.sum(res == gt) / (res.shape[0]*res.shape[1])\n",
    "\n",
    "            loss_bank.append(loss.item())\n",
    "            dice_bank.append(dice)\n",
    "            iou_bank.append(iou)\n",
    "            acc_bank.append(acc)\n",
    "            \n",
    "        print('{} Loss: {:.4f}, Dice: {:.4f}, IoU: {:.4f}, Acc: {:.4f}'.\n",
    "            format(s, np.mean(loss_bank), np.mean(dice_bank), np.mean(iou_bank), np.mean(acc_bank)))\n",
    "\n",
    "        mean_loss.append(np.mean(loss_bank))\n",
    "\n",
    "    return mean_loss[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf9dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Start Training ####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 18:35:29.750390 Epoch [001/025], Step [0020/0125], [lateral-2: 1.3417, lateral-3: 1.2493, lateral-4: 1.3089]\n",
      "2022-08-29 18:35:37.103439 Epoch [001/025], Step [0040/0125], [lateral-2: 1.1359, lateral-3: 1.0878, lateral-4: 1.1321]\n",
      "2022-08-29 18:35:44.436538 Epoch [001/025], Step [0060/0125], [lateral-2: 0.8667, lateral-3: 0.8754, lateral-4: 0.8926]\n",
      "2022-08-29 18:35:51.652452 Epoch [001/025], Step [0080/0125], [lateral-2: 0.7747, lateral-3: 0.7921, lateral-4: 0.7948]\n",
      "2022-08-29 18:35:58.963782 Epoch [001/025], Step [0100/0125], [lateral-2: 0.7012, lateral-3: 0.7195, lateral-4: 0.7164]\n",
      "2022-08-29 18:36:06.237108 Epoch [001/025], Step [0120/0125], [lateral-2: 0.6467, lateral-3: 0.6678, lateral-4: 0.6643]\n",
      "2022-08-29 18:36:08.096614 Epoch [001/025], Step [0125/0125], [lateral-2: 0.6420, lateral-3: 0.6629, lateral-4: 0.6599]\n",
      "val Loss: 0.6109, Dice: 0.8435, IoU: 0.7550, Acc: 0.9520\n",
      "test Loss: 0.6973, Dice: 0.8424, IoU: 0.7528, Acc: 0.9330\n",
      "new best loss:  0.6109491238441037\n",
      "[Saving Snapshot:] snapshots/TransFuse_S/TransFuse-1.pth\n",
      "2022-08-29 18:36:32.627636 Epoch [002/025], Step [0020/0125], [lateral-2: 0.6302, lateral-3: 0.6527, lateral-4: 0.6434]\n",
      "2022-08-29 18:36:40.717613 Epoch [002/025], Step [0040/0125], [lateral-2: 0.6009, lateral-3: 0.6192, lateral-4: 0.6163]\n",
      "2022-08-29 18:36:48.513014 Epoch [002/025], Step [0060/0125], [lateral-2: 0.5511, lateral-3: 0.5706, lateral-4: 0.5726]\n",
      "2022-08-29 18:36:56.348093 Epoch [002/025], Step [0080/0125], [lateral-2: 0.5363, lateral-3: 0.5615, lateral-4: 0.5606]\n",
      "2022-08-29 18:37:04.412921 Epoch [002/025], Step [0100/0125], [lateral-2: 0.5350, lateral-3: 0.5577, lateral-4: 0.5584]\n",
      "2022-08-29 18:37:12.149434 Epoch [002/025], Step [0120/0125], [lateral-2: 0.5151, lateral-3: 0.5418, lateral-4: 0.5392]\n",
      "2022-08-29 18:37:14.088672 Epoch [002/025], Step [0125/0125], [lateral-2: 0.5124, lateral-3: 0.5415, lateral-4: 0.5380]\n",
      "val Loss: 0.5751, Dice: 0.8334, IoU: 0.7481, Acc: 0.9481\n",
      "test Loss: 0.7373, Dice: 0.8158, IoU: 0.7260, Acc: 0.9234\n",
      "new best loss:  0.5750775501802158\n",
      "[Saving Snapshot:] snapshots/TransFuse_S/TransFuse-2.pth\n",
      "2022-08-29 18:37:38.425003 Epoch [003/025], Step [0020/0125], [lateral-2: 0.4660, lateral-3: 0.5039, lateral-4: 0.4983]\n",
      "2022-08-29 18:37:46.293862 Epoch [003/025], Step [0040/0125], [lateral-2: 0.4878, lateral-3: 0.5259, lateral-4: 0.5183]\n",
      "2022-08-29 18:37:54.034223 Epoch [003/025], Step [0060/0125], [lateral-2: 0.5012, lateral-3: 0.5362, lateral-4: 0.5294]\n",
      "2022-08-29 18:38:01.851081 Epoch [003/025], Step [0080/0125], [lateral-2: 0.4720, lateral-3: 0.5031, lateral-4: 0.5004]\n",
      "2022-08-29 18:38:09.643246 Epoch [003/025], Step [0100/0125], [lateral-2: 0.4529, lateral-3: 0.4896, lateral-4: 0.4846]\n",
      "2022-08-29 18:38:17.360540 Epoch [003/025], Step [0120/0125], [lateral-2: 0.4542, lateral-3: 0.4935, lateral-4: 0.4869]\n",
      "2022-08-29 18:38:19.288738 Epoch [003/025], Step [0125/0125], [lateral-2: 0.4487, lateral-3: 0.4887, lateral-4: 0.4811]\n",
      "val Loss: 0.5211, Dice: 0.8634, IoU: 0.7814, Acc: 0.9550\n",
      "test Loss: 0.6392, Dice: 0.8484, IoU: 0.7628, Acc: 0.9347\n",
      "new best loss:  0.5210765284649755\n",
      "[Saving Snapshot:] snapshots/TransFuse_S/TransFuse-3.pth\n",
      "2022-08-29 18:38:44.274447 Epoch [004/025], Step [0020/0125], [lateral-2: 0.4506, lateral-3: 0.4928, lateral-4: 0.4826]\n",
      "2022-08-29 18:38:52.390540 Epoch [004/025], Step [0040/0125], [lateral-2: 0.4452, lateral-3: 0.4840, lateral-4: 0.4776]\n",
      "2022-08-29 18:39:00.497480 Epoch [004/025], Step [0060/0125], [lateral-2: 0.4312, lateral-3: 0.4726, lateral-4: 0.4686]\n",
      "2022-08-29 18:39:08.402339 Epoch [004/025], Step [0080/0125], [lateral-2: 0.4437, lateral-3: 0.4841, lateral-4: 0.4812]\n",
      "2022-08-29 18:39:16.405396 Epoch [004/025], Step [0100/0125], [lateral-2: 0.4391, lateral-3: 0.4776, lateral-4: 0.4746]\n",
      "2022-08-29 18:39:24.360905 Epoch [004/025], Step [0120/0125], [lateral-2: 0.4247, lateral-3: 0.4649, lateral-4: 0.4622]\n",
      "2022-08-29 18:39:26.321150 Epoch [004/025], Step [0125/0125], [lateral-2: 0.4236, lateral-3: 0.4645, lateral-4: 0.4609]\n",
      "val Loss: 0.4817, Dice: 0.8698, IoU: 0.7904, Acc: 0.9586\n",
      "test Loss: 0.6687, Dice: 0.8486, IoU: 0.7638, Acc: 0.9350\n",
      "new best loss:  0.481675213112959\n",
      "[Saving Snapshot:] snapshots/TransFuse_S/TransFuse-4.pth\n",
      "2022-08-29 18:39:51.848746 Epoch [005/025], Step [0020/0125], [lateral-2: 0.4235, lateral-3: 0.4607, lateral-4: 0.4569]\n",
      "2022-08-29 18:39:59.964153 Epoch [005/025], Step [0040/0125], [lateral-2: 0.4150, lateral-3: 0.4551, lateral-4: 0.4520]\n",
      "2022-08-29 18:40:07.979240 Epoch [005/025], Step [0060/0125], [lateral-2: 0.3958, lateral-3: 0.4369, lateral-4: 0.4339]\n",
      "2022-08-29 18:40:15.853530 Epoch [005/025], Step [0080/0125], [lateral-2: 0.3943, lateral-3: 0.4333, lateral-4: 0.4287]\n",
      "2022-08-29 18:40:23.760707 Epoch [005/025], Step [0100/0125], [lateral-2: 0.4082, lateral-3: 0.4498, lateral-4: 0.4432]\n",
      "2022-08-29 18:40:31.775365 Epoch [005/025], Step [0120/0125], [lateral-2: 0.4141, lateral-3: 0.4573, lateral-4: 0.4515]\n",
      "2022-08-29 18:40:33.725181 Epoch [005/025], Step [0125/0125], [lateral-2: 0.4045, lateral-3: 0.4488, lateral-4: 0.4433]\n",
      "val Loss: 0.4894, Dice: 0.8680, IoU: 0.7879, Acc: 0.9563\n",
      "test Loss: 0.6549, Dice: 0.8490, IoU: 0.7659, Acc: 0.9347\n",
      "2022-08-29 18:40:59.801229 Epoch [006/025], Step [0020/0125], [lateral-2: 0.3903, lateral-3: 0.4351, lateral-4: 0.4279]\n",
      "2022-08-29 18:41:07.724811 Epoch [006/025], Step [0040/0125], [lateral-2: 0.3819, lateral-3: 0.4235, lateral-4: 0.4180]\n",
      "2022-08-29 18:41:15.963372 Epoch [006/025], Step [0060/0125], [lateral-2: 0.3826, lateral-3: 0.4195, lateral-4: 0.4159]\n",
      "2022-08-29 18:41:24.142576 Epoch [006/025], Step [0080/0125], [lateral-2: 0.3901, lateral-3: 0.4294, lateral-4: 0.4238]\n",
      "2022-08-29 18:41:32.424414 Epoch [006/025], Step [0100/0125], [lateral-2: 0.3877, lateral-3: 0.4268, lateral-4: 0.4235]\n",
      "2022-08-29 18:41:40.494728 Epoch [006/025], Step [0120/0125], [lateral-2: 0.3962, lateral-3: 0.4349, lateral-4: 0.4326]\n",
      "2022-08-29 18:41:42.556502 Epoch [006/025], Step [0125/0125], [lateral-2: 0.3986, lateral-3: 0.4385, lateral-4: 0.4337]\n",
      "val Loss: 0.5600, Dice: 0.8545, IoU: 0.7750, Acc: 0.9496\n",
      "test Loss: 0.7602, Dice: 0.8361, IoU: 0.7514, Acc: 0.9279\n",
      "2022-08-29 18:42:10.178995 Epoch [007/025], Step [0020/0125], [lateral-2: 0.3582, lateral-3: 0.3931, lateral-4: 0.3927]\n",
      "2022-08-29 18:42:18.701280 Epoch [007/025], Step [0040/0125], [lateral-2: 0.3694, lateral-3: 0.4027, lateral-4: 0.4032]\n",
      "2022-08-29 18:42:26.749927 Epoch [007/025], Step [0060/0125], [lateral-2: 0.3827, lateral-3: 0.4165, lateral-4: 0.4177]\n",
      "2022-08-29 18:42:34.734972 Epoch [007/025], Step [0080/0125], [lateral-2: 0.3917, lateral-3: 0.4281, lateral-4: 0.4273]\n",
      "2022-08-29 18:42:43.052755 Epoch [007/025], Step [0100/0125], [lateral-2: 0.3901, lateral-3: 0.4255, lateral-4: 0.4234]\n",
      "2022-08-29 18:42:51.012286 Epoch [007/025], Step [0120/0125], [lateral-2: 0.3705, lateral-3: 0.4076, lateral-4: 0.4062]\n",
      "2022-08-29 18:42:53.009160 Epoch [007/025], Step [0125/0125], [lateral-2: 0.3737, lateral-3: 0.4101, lateral-4: 0.4089]\n",
      "val Loss: 0.4702, Dice: 0.8764, IoU: 0.7987, Acc: 0.9616\n",
      "test Loss: 0.6608, Dice: 0.8504, IoU: 0.7663, Acc: 0.9392\n",
      "new best loss:  0.4701507025899069\n",
      "[Saving Snapshot:] snapshots/TransFuse_S/TransFuse-7.pth\n",
      "2022-08-29 18:43:19.170814 Epoch [008/025], Step [0020/0125], [lateral-2: 0.3881, lateral-3: 0.4246, lateral-4: 0.4202]\n",
      "2022-08-29 18:43:27.706256 Epoch [008/025], Step [0040/0125], [lateral-2: 0.3862, lateral-3: 0.4245, lateral-4: 0.4190]\n",
      "2022-08-29 18:43:35.784764 Epoch [008/025], Step [0060/0125], [lateral-2: 0.3730, lateral-3: 0.4122, lateral-4: 0.4068]\n",
      "2022-08-29 18:43:43.842821 Epoch [008/025], Step [0080/0125], [lateral-2: 0.3632, lateral-3: 0.4009, lateral-4: 0.3987]\n",
      "2022-08-29 18:43:51.727142 Epoch [008/025], Step [0100/0125], [lateral-2: 0.3611, lateral-3: 0.3966, lateral-4: 0.3980]\n",
      "2022-08-29 18:43:59.594990 Epoch [008/025], Step [0120/0125], [lateral-2: 0.3585, lateral-3: 0.3939, lateral-4: 0.3940]\n",
      "2022-08-29 18:44:01.614614 Epoch [008/025], Step [0125/0125], [lateral-2: 0.3563, lateral-3: 0.3923, lateral-4: 0.3913]\n",
      "val Loss: 0.5417, Dice: 0.8605, IoU: 0.7850, Acc: 0.9524\n",
      "test Loss: 0.7394, Dice: 0.8437, IoU: 0.7583, Acc: 0.9342\n",
      "2022-08-29 18:44:27.919865 Epoch [009/025], Step [0020/0125], [lateral-2: 0.3404, lateral-3: 0.3841, lateral-4: 0.3790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 18:44:36.365568 Epoch [009/025], Step [0040/0125], [lateral-2: 0.3586, lateral-3: 0.3952, lateral-4: 0.3942]\n",
      "2022-08-29 18:44:45.373679 Epoch [009/025], Step [0060/0125], [lateral-2: 0.3760, lateral-3: 0.4116, lateral-4: 0.4084]\n",
      "2022-08-29 18:44:54.086894 Epoch [009/025], Step [0080/0125], [lateral-2: 0.3599, lateral-3: 0.4000, lateral-4: 0.3941]\n",
      "2022-08-29 18:45:02.869919 Epoch [009/025], Step [0100/0125], [lateral-2: 0.3564, lateral-3: 0.3953, lateral-4: 0.3906]\n",
      "2022-08-29 18:45:11.771436 Epoch [009/025], Step [0120/0125], [lateral-2: 0.3588, lateral-3: 0.3948, lateral-4: 0.3902]\n",
      "2022-08-29 18:45:13.907909 Epoch [009/025], Step [0125/0125], [lateral-2: 0.3554, lateral-3: 0.3917, lateral-4: 0.3881]\n",
      "val Loss: 0.5275, Dice: 0.8684, IoU: 0.7916, Acc: 0.9564\n",
      "test Loss: 0.7330, Dice: 0.8459, IoU: 0.7635, Acc: 0.9364\n",
      "2022-08-29 18:45:41.144524 Epoch [010/025], Step [0020/0125], [lateral-2: 0.3601, lateral-3: 0.3987, lateral-4: 0.3944]\n",
      "2022-08-29 18:45:49.736500 Epoch [010/025], Step [0040/0125], [lateral-2: 0.3528, lateral-3: 0.3890, lateral-4: 0.3859]\n",
      "2022-08-29 18:45:58.337266 Epoch [010/025], Step [0060/0125], [lateral-2: 0.3554, lateral-3: 0.3894, lateral-4: 0.3861]\n",
      "2022-08-29 18:46:06.845105 Epoch [010/025], Step [0080/0125], [lateral-2: 0.3582, lateral-3: 0.3924, lateral-4: 0.3885]\n",
      "2022-08-29 18:46:14.820579 Epoch [010/025], Step [0100/0125], [lateral-2: 0.3515, lateral-3: 0.3871, lateral-4: 0.3827]\n",
      "2022-08-29 18:46:22.688373 Epoch [010/025], Step [0120/0125], [lateral-2: 0.3451, lateral-3: 0.3777, lateral-4: 0.3769]\n",
      "2022-08-29 18:46:24.649749 Epoch [010/025], Step [0125/0125], [lateral-2: 0.3444, lateral-3: 0.3783, lateral-4: 0.3768]\n",
      "val Loss: 0.4935, Dice: 0.8715, IoU: 0.7944, Acc: 0.9568\n",
      "test Loss: 0.7089, Dice: 0.8506, IoU: 0.7682, Acc: 0.9351\n",
      "2022-08-29 18:46:49.670489 Epoch [011/025], Step [0020/0125], [lateral-2: 0.3465, lateral-3: 0.3855, lateral-4: 0.3825]\n",
      "2022-08-29 18:46:57.839821 Epoch [011/025], Step [0040/0125], [lateral-2: 0.3466, lateral-3: 0.3834, lateral-4: 0.3782]\n",
      "2022-08-29 18:47:06.112867 Epoch [011/025], Step [0060/0125], [lateral-2: 0.3531, lateral-3: 0.3908, lateral-4: 0.3845]\n",
      "2022-08-29 18:47:14.217704 Epoch [011/025], Step [0080/0125], [lateral-2: 0.3545, lateral-3: 0.3916, lateral-4: 0.3887]\n",
      "2022-08-29 18:47:22.170336 Epoch [011/025], Step [0100/0125], [lateral-2: 0.3460, lateral-3: 0.3785, lateral-4: 0.3770]\n",
      "2022-08-29 18:47:30.278162 Epoch [011/025], Step [0120/0125], [lateral-2: 0.3323, lateral-3: 0.3613, lateral-4: 0.3618]\n",
      "2022-08-29 18:47:32.304993 Epoch [011/025], Step [0125/0125], [lateral-2: 0.3357, lateral-3: 0.3652, lateral-4: 0.3646]\n",
      "val Loss: 0.4889, Dice: 0.8693, IoU: 0.7931, Acc: 0.9569\n",
      "test Loss: 0.6575, Dice: 0.8587, IoU: 0.7802, Acc: 0.9409\n",
      "2022-08-29 18:47:57.974764 Epoch [012/025], Step [0020/0125], [lateral-2: 0.3264, lateral-3: 0.3634, lateral-4: 0.3618]\n",
      "2022-08-29 18:48:06.279476 Epoch [012/025], Step [0040/0125], [lateral-2: 0.3278, lateral-3: 0.3617, lateral-4: 0.3632]\n",
      "2022-08-29 18:48:14.483427 Epoch [012/025], Step [0060/0125], [lateral-2: 0.3350, lateral-3: 0.3669, lateral-4: 0.3676]\n",
      "2022-08-29 18:48:22.841185 Epoch [012/025], Step [0080/0125], [lateral-2: 0.3534, lateral-3: 0.3825, lateral-4: 0.3814]\n",
      "2022-08-29 18:48:31.289595 Epoch [012/025], Step [0100/0125], [lateral-2: 0.3484, lateral-3: 0.3810, lateral-4: 0.3784]\n",
      "2022-08-29 18:48:39.292363 Epoch [012/025], Step [0120/0125], [lateral-2: 0.3322, lateral-3: 0.3679, lateral-4: 0.3630]\n",
      "2022-08-29 18:48:41.229860 Epoch [012/025], Step [0125/0125], [lateral-2: 0.3291, lateral-3: 0.3646, lateral-4: 0.3600]\n",
      "val Loss: 0.4708, Dice: 0.8765, IoU: 0.8012, Acc: 0.9593\n",
      "test Loss: 0.6795, Dice: 0.8583, IoU: 0.7776, Acc: 0.9384\n",
      "2022-08-29 18:49:07.023209 Epoch [013/025], Step [0020/0125], [lateral-2: 0.3222, lateral-3: 0.3554, lateral-4: 0.3514]\n",
      "2022-08-29 18:49:15.082786 Epoch [013/025], Step [0040/0125], [lateral-2: 0.3381, lateral-3: 0.3708, lateral-4: 0.3675]\n",
      "2022-08-29 18:49:23.155857 Epoch [013/025], Step [0060/0125], [lateral-2: 0.3303, lateral-3: 0.3627, lateral-4: 0.3615]\n",
      "2022-08-29 18:49:31.472187 Epoch [013/025], Step [0080/0125], [lateral-2: 0.3189, lateral-3: 0.3525, lateral-4: 0.3518]\n",
      "2022-08-29 18:49:39.737542 Epoch [013/025], Step [0100/0125], [lateral-2: 0.3298, lateral-3: 0.3626, lateral-4: 0.3606]\n",
      "2022-08-29 18:49:47.813594 Epoch [013/025], Step [0120/0125], [lateral-2: 0.3270, lateral-3: 0.3574, lateral-4: 0.3558]\n",
      "2022-08-29 18:49:49.955314 Epoch [013/025], Step [0125/0125], [lateral-2: 0.3250, lateral-3: 0.3557, lateral-4: 0.3550]\n",
      "val Loss: 0.5015, Dice: 0.8742, IoU: 0.7999, Acc: 0.9551\n",
      "test Loss: 0.6990, Dice: 0.8558, IoU: 0.7757, Acc: 0.9394\n",
      "2022-08-29 18:50:16.983085 Epoch [014/025], Step [0020/0125], [lateral-2: 0.3330, lateral-3: 0.3683, lateral-4: 0.3650]\n",
      "2022-08-29 18:50:25.563135 Epoch [014/025], Step [0040/0125], [lateral-2: 0.3249, lateral-3: 0.3597, lateral-4: 0.3577]\n",
      "2022-08-29 18:50:33.900526 Epoch [014/025], Step [0060/0125], [lateral-2: 0.3207, lateral-3: 0.3545, lateral-4: 0.3515]\n",
      "2022-08-29 18:50:41.952860 Epoch [014/025], Step [0080/0125], [lateral-2: 0.3333, lateral-3: 0.3654, lateral-4: 0.3623]\n",
      "2022-08-29 18:50:50.094600 Epoch [014/025], Step [0100/0125], [lateral-2: 0.3314, lateral-3: 0.3615, lateral-4: 0.3608]\n",
      "2022-08-29 18:50:57.910149 Epoch [014/025], Step [0120/0125], [lateral-2: 0.3299, lateral-3: 0.3589, lateral-4: 0.3584]\n",
      "2022-08-29 18:50:59.914489 Epoch [014/025], Step [0125/0125], [lateral-2: 0.3319, lateral-3: 0.3607, lateral-4: 0.3610]\n",
      "val Loss: 0.5242, Dice: 0.8629, IoU: 0.7853, Acc: 0.9483\n",
      "test Loss: 0.6459, Dice: 0.8642, IoU: 0.7851, Acc: 0.9406\n",
      "2022-08-29 18:51:26.329336 Epoch [015/025], Step [0020/0125], [lateral-2: 0.3126, lateral-3: 0.3396, lateral-4: 0.3383]\n",
      "2022-08-29 18:51:34.296196 Epoch [015/025], Step [0040/0125], [lateral-2: 0.3105, lateral-3: 0.3394, lateral-4: 0.3380]\n",
      "2022-08-29 18:51:42.184551 Epoch [015/025], Step [0060/0125], [lateral-2: 0.3212, lateral-3: 0.3544, lateral-4: 0.3499]\n",
      "2022-08-29 18:51:50.049697 Epoch [015/025], Step [0080/0125], [lateral-2: 0.3405, lateral-3: 0.3737, lateral-4: 0.3704]\n",
      "2022-08-29 18:51:57.888301 Epoch [015/025], Step [0100/0125], [lateral-2: 0.3279, lateral-3: 0.3612, lateral-4: 0.3597]\n",
      "2022-08-29 18:52:05.732116 Epoch [015/025], Step [0120/0125], [lateral-2: 0.3230, lateral-3: 0.3550, lateral-4: 0.3544]\n",
      "2022-08-29 18:52:07.693463 Epoch [015/025], Step [0125/0125], [lateral-2: 0.3270, lateral-3: 0.3572, lateral-4: 0.3572]\n",
      "val Loss: 0.4812, Dice: 0.8771, IoU: 0.8019, Acc: 0.9597\n",
      "test Loss: 0.7097, Dice: 0.8527, IoU: 0.7683, Acc: 0.9388\n",
      "2022-08-29 18:52:34.408417 Epoch [016/025], Step [0020/0125], [lateral-2: 0.3194, lateral-3: 0.3495, lateral-4: 0.3490]\n",
      "2022-08-29 18:52:42.398161 Epoch [016/025], Step [0040/0125], [lateral-2: 0.3265, lateral-3: 0.3575, lateral-4: 0.3571]\n",
      "2022-08-29 18:52:50.264612 Epoch [016/025], Step [0060/0125], [lateral-2: 0.3232, lateral-3: 0.3563, lateral-4: 0.3539]\n",
      "2022-08-29 18:52:58.361475 Epoch [016/025], Step [0080/0125], [lateral-2: 0.3020, lateral-3: 0.3356, lateral-4: 0.3314]\n",
      "2022-08-29 18:53:06.318690 Epoch [016/025], Step [0100/0125], [lateral-2: 0.3044, lateral-3: 0.3343, lateral-4: 0.3323]\n",
      "2022-08-29 18:53:14.197085 Epoch [016/025], Step [0120/0125], [lateral-2: 0.3149, lateral-3: 0.3420, lateral-4: 0.3429]\n",
      "2022-08-29 18:53:16.148230 Epoch [016/025], Step [0125/0125], [lateral-2: 0.3153, lateral-3: 0.3430, lateral-4: 0.3436]\n",
      "val Loss: 0.5637, Dice: 0.8593, IoU: 0.7837, Acc: 0.9517\n",
      "test Loss: 0.7785, Dice: 0.8445, IoU: 0.7629, Acc: 0.9342\n",
      "2022-08-29 18:53:42.143951 Epoch [017/025], Step [0020/0125], [lateral-2: 0.3035, lateral-3: 0.3353, lateral-4: 0.3325]\n",
      "2022-08-29 18:53:50.318350 Epoch [017/025], Step [0040/0125], [lateral-2: 0.3101, lateral-3: 0.3410, lateral-4: 0.3381]\n",
      "2022-08-29 18:53:58.377481 Epoch [017/025], Step [0060/0125], [lateral-2: 0.3143, lateral-3: 0.3449, lateral-4: 0.3418]\n",
      "2022-08-29 18:54:06.252568 Epoch [017/025], Step [0080/0125], [lateral-2: 0.3134, lateral-3: 0.3451, lateral-4: 0.3442]\n",
      "2022-08-29 18:54:14.008600 Epoch [017/025], Step [0100/0125], [lateral-2: 0.3208, lateral-3: 0.3520, lateral-4: 0.3503]\n",
      "2022-08-29 18:54:21.925906 Epoch [017/025], Step [0120/0125], [lateral-2: 0.3119, lateral-3: 0.3416, lateral-4: 0.3420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 18:54:23.858840 Epoch [017/025], Step [0125/0125], [lateral-2: 0.3172, lateral-3: 0.3469, lateral-4: 0.3479]\n",
      "val Loss: 0.4876, Dice: 0.8762, IoU: 0.8029, Acc: 0.9570\n",
      "test Loss: 0.7057, Dice: 0.8521, IoU: 0.7699, Acc: 0.9381\n",
      "2022-08-29 18:54:48.979349 Epoch [018/025], Step [0020/0125], [lateral-2: 0.3083, lateral-3: 0.3413, lateral-4: 0.3391]\n",
      "2022-08-29 18:54:56.976520 Epoch [018/025], Step [0040/0125], [lateral-2: 0.3104, lateral-3: 0.3420, lateral-4: 0.3402]\n",
      "2022-08-29 18:55:04.938076 Epoch [018/025], Step [0060/0125], [lateral-2: 0.3146, lateral-3: 0.3423, lateral-4: 0.3443]\n",
      "2022-08-29 18:55:12.766720 Epoch [018/025], Step [0080/0125], [lateral-2: 0.3067, lateral-3: 0.3325, lateral-4: 0.3372]\n",
      "2022-08-29 18:55:20.699003 Epoch [018/025], Step [0100/0125], [lateral-2: 0.3017, lateral-3: 0.3291, lateral-4: 0.3313]\n",
      "2022-08-29 18:55:28.498234 Epoch [018/025], Step [0120/0125], [lateral-2: 0.3067, lateral-3: 0.3360, lateral-4: 0.3350]\n",
      "2022-08-29 18:55:30.431734 Epoch [018/025], Step [0125/0125], [lateral-2: 0.3088, lateral-3: 0.3372, lateral-4: 0.3364]\n",
      "val Loss: 0.5209, Dice: 0.8734, IoU: 0.7983, Acc: 0.9577\n",
      "test Loss: 0.7642, Dice: 0.8488, IoU: 0.7657, Acc: 0.9367\n",
      "2022-08-29 18:55:55.085495 Epoch [019/025], Step [0020/0125], [lateral-2: 0.3042, lateral-3: 0.3373, lateral-4: 0.3358]\n",
      "2022-08-29 18:56:03.202605 Epoch [019/025], Step [0040/0125], [lateral-2: 0.3006, lateral-3: 0.3338, lateral-4: 0.3306]\n",
      "2022-08-29 18:56:11.135009 Epoch [019/025], Step [0060/0125], [lateral-2: 0.3166, lateral-3: 0.3476, lateral-4: 0.3434]\n",
      "2022-08-29 18:56:19.042051 Epoch [019/025], Step [0080/0125], [lateral-2: 0.3285, lateral-3: 0.3569, lateral-4: 0.3536]\n",
      "2022-08-29 18:56:27.136245 Epoch [019/025], Step [0100/0125], [lateral-2: 0.3034, lateral-3: 0.3327, lateral-4: 0.3314]\n",
      "2022-08-29 18:56:35.138300 Epoch [019/025], Step [0120/0125], [lateral-2: 0.2925, lateral-3: 0.3264, lateral-4: 0.3239]\n",
      "2022-08-29 18:56:37.053608 Epoch [019/025], Step [0125/0125], [lateral-2: 0.2950, lateral-3: 0.3297, lateral-4: 0.3254]\n",
      "val Loss: 0.5029, Dice: 0.8768, IoU: 0.8042, Acc: 0.9579\n",
      "test Loss: 0.7469, Dice: 0.8549, IoU: 0.7735, Acc: 0.9394\n",
      "2022-08-29 18:57:02.224015 Epoch [020/025], Step [0020/0125], [lateral-2: 0.2963, lateral-3: 0.3221, lateral-4: 0.3237]\n",
      "2022-08-29 18:57:10.140982 Epoch [020/025], Step [0040/0125], [lateral-2: 0.2983, lateral-3: 0.3251, lateral-4: 0.3253]\n",
      "2022-08-29 18:57:18.250381 Epoch [020/025], Step [0060/0125], [lateral-2: 0.2991, lateral-3: 0.3298, lateral-4: 0.3282]\n",
      "2022-08-29 18:57:26.019158 Epoch [020/025], Step [0080/0125], [lateral-2: 0.3073, lateral-3: 0.3362, lateral-4: 0.3367]\n",
      "2022-08-29 18:57:33.932166 Epoch [020/025], Step [0100/0125], [lateral-2: 0.2987, lateral-3: 0.3255, lateral-4: 0.3284]\n",
      "2022-08-29 18:57:41.788560 Epoch [020/025], Step [0120/0125], [lateral-2: 0.2886, lateral-3: 0.3196, lateral-4: 0.3187]\n",
      "2022-08-29 18:57:43.774818 Epoch [020/025], Step [0125/0125], [lateral-2: 0.2911, lateral-3: 0.3222, lateral-4: 0.3210]\n",
      "val Loss: 0.5681, Dice: 0.8676, IoU: 0.7923, Acc: 0.9568\n",
      "test Loss: 0.8442, Dice: 0.8396, IoU: 0.7515, Acc: 0.9347\n",
      "2022-08-29 18:58:09.412448 Epoch [021/025], Step [0020/0125], [lateral-2: 0.3000, lateral-3: 0.3256, lateral-4: 0.3277]\n",
      "2022-08-29 18:58:17.383313 Epoch [021/025], Step [0040/0125], [lateral-2: 0.2963, lateral-3: 0.3259, lateral-4: 0.3244]\n",
      "2022-08-29 18:58:25.344818 Epoch [021/025], Step [0060/0125], [lateral-2: 0.2985, lateral-3: 0.3287, lateral-4: 0.3259]\n",
      "2022-08-29 18:58:33.242947 Epoch [021/025], Step [0080/0125], [lateral-2: 0.3021, lateral-3: 0.3313, lateral-4: 0.3283]\n",
      "2022-08-29 18:58:41.180355 Epoch [021/025], Step [0100/0125], [lateral-2: 0.3005, lateral-3: 0.3282, lateral-4: 0.3276]\n",
      "2022-08-29 18:58:49.218250 Epoch [021/025], Step [0120/0125], [lateral-2: 0.3063, lateral-3: 0.3298, lateral-4: 0.3325]\n",
      "2022-08-29 18:58:51.269713 Epoch [021/025], Step [0125/0125], [lateral-2: 0.3134, lateral-3: 0.3376, lateral-4: 0.3386]\n",
      "val Loss: 0.4931, Dice: 0.8777, IoU: 0.8040, Acc: 0.9603\n",
      "test Loss: 0.7172, Dice: 0.8584, IoU: 0.7777, Acc: 0.9399\n",
      "2022-08-29 18:59:17.952720 Epoch [022/025], Step [0020/0125], [lateral-2: 0.2992, lateral-3: 0.3287, lateral-4: 0.3285]\n",
      "2022-08-29 18:59:25.903407 Epoch [022/025], Step [0040/0125], [lateral-2: 0.2985, lateral-3: 0.3281, lateral-4: 0.3277]\n",
      "2022-08-29 18:59:33.812510 Epoch [022/025], Step [0060/0125], [lateral-2: 0.2871, lateral-3: 0.3175, lateral-4: 0.3171]\n",
      "2022-08-29 18:59:41.776021 Epoch [022/025], Step [0080/0125], [lateral-2: 0.2783, lateral-3: 0.3095, lateral-4: 0.3092]\n",
      "2022-08-29 18:59:49.609231 Epoch [022/025], Step [0100/0125], [lateral-2: 0.2899, lateral-3: 0.3214, lateral-4: 0.3177]\n",
      "2022-08-29 18:59:57.441292 Epoch [022/025], Step [0120/0125], [lateral-2: 0.2924, lateral-3: 0.3213, lateral-4: 0.3190]\n",
      "2022-08-29 18:59:59.438254 Epoch [022/025], Step [0125/0125], [lateral-2: 0.2920, lateral-3: 0.3206, lateral-4: 0.3183]\n",
      "val Loss: 0.5876, Dice: 0.8685, IoU: 0.7939, Acc: 0.9551\n",
      "test Loss: 0.8679, Dice: 0.8396, IoU: 0.7551, Acc: 0.9334\n",
      "2022-08-29 19:00:24.692718 Epoch [023/025], Step [0020/0125], [lateral-2: 0.2846, lateral-3: 0.3120, lateral-4: 0.3126]\n",
      "2022-08-29 19:00:32.591845 Epoch [023/025], Step [0040/0125], [lateral-2: 0.2975, lateral-3: 0.3259, lateral-4: 0.3251]\n",
      "2022-08-29 19:00:40.620395 Epoch [023/025], Step [0060/0125], [lateral-2: 0.3084, lateral-3: 0.3395, lateral-4: 0.3360]\n",
      "2022-08-29 19:00:48.482088 Epoch [023/025], Step [0080/0125], [lateral-2: 0.2903, lateral-3: 0.3213, lateral-4: 0.3182]\n",
      "2022-08-29 19:00:56.325841 Epoch [023/025], Step [0100/0125], [lateral-2: 0.2775, lateral-3: 0.3073, lateral-4: 0.3064]\n",
      "2022-08-29 19:01:04.314191 Epoch [023/025], Step [0120/0125], [lateral-2: 0.2933, lateral-3: 0.3232, lateral-4: 0.3206]\n",
      "2022-08-29 19:01:06.281538 Epoch [023/025], Step [0125/0125], [lateral-2: 0.2939, lateral-3: 0.3240, lateral-4: 0.3204]\n",
      "val Loss: 0.5109, Dice: 0.8749, IoU: 0.8002, Acc: 0.9573\n",
      "test Loss: 0.7472, Dice: 0.8558, IoU: 0.7752, Acc: 0.9389\n",
      "2022-08-29 19:01:31.185021 Epoch [024/025], Step [0020/0125], [lateral-2: 0.2913, lateral-3: 0.3182, lateral-4: 0.3198]\n",
      "2022-08-29 19:01:39.159518 Epoch [024/025], Step [0040/0125], [lateral-2: 0.2889, lateral-3: 0.3158, lateral-4: 0.3172]\n",
      "2022-08-29 19:01:47.011480 Epoch [024/025], Step [0060/0125], [lateral-2: 0.2925, lateral-3: 0.3201, lateral-4: 0.3191]\n",
      "2022-08-29 19:01:54.908176 Epoch [024/025], Step [0080/0125], [lateral-2: 0.2992, lateral-3: 0.3265, lateral-4: 0.3248]\n",
      "2022-08-29 19:02:02.831588 Epoch [024/025], Step [0100/0125], [lateral-2: 0.2883, lateral-3: 0.3144, lateral-4: 0.3170]\n",
      "2022-08-29 19:02:10.566783 Epoch [024/025], Step [0120/0125], [lateral-2: 0.2771, lateral-3: 0.3055, lateral-4: 0.3075]\n",
      "2022-08-29 19:02:12.488889 Epoch [024/025], Step [0125/0125], [lateral-2: 0.2723, lateral-3: 0.3015, lateral-4: 0.3037]\n",
      "val Loss: 0.5020, Dice: 0.8783, IoU: 0.8038, Acc: 0.9599\n",
      "test Loss: 0.7161, Dice: 0.8556, IoU: 0.7740, Acc: 0.9427\n",
      "2022-08-29 19:02:38.104399 Epoch [025/025], Step [0020/0125], [lateral-2: 0.2903, lateral-3: 0.3134, lateral-4: 0.3141]\n",
      "2022-08-29 19:02:45.955392 Epoch [025/025], Step [0040/0125], [lateral-2: 0.2913, lateral-3: 0.3187, lateral-4: 0.3183]\n",
      "2022-08-29 19:02:53.755636 Epoch [025/025], Step [0060/0125], [lateral-2: 0.2823, lateral-3: 0.3131, lateral-4: 0.3129]\n",
      "2022-08-29 19:03:01.555592 Epoch [025/025], Step [0080/0125], [lateral-2: 0.2799, lateral-3: 0.3080, lateral-4: 0.3106]\n",
      "2022-08-29 19:03:09.469047 Epoch [025/025], Step [0100/0125], [lateral-2: 0.2748, lateral-3: 0.3009, lateral-4: 0.3026]\n",
      "2022-08-29 19:03:17.400084 Epoch [025/025], Step [0120/0125], [lateral-2: 0.2692, lateral-3: 0.2969, lateral-4: 0.2968]\n",
      "2022-08-29 19:03:19.447538 Epoch [025/025], Step [0125/0125], [lateral-2: 0.2720, lateral-3: 0.2994, lateral-4: 0.2995]\n",
      "val Loss: 0.4814, Dice: 0.8796, IoU: 0.8045, Acc: 0.9606\n",
      "test Loss: 0.7341, Dice: 0.8588, IoU: 0.7758, Acc: 0.9408\n"
     ]
    }
   ],
   "source": [
    "# ---- build models ----\n",
    "model = TransFuse_S(pretrained=True).cuda()\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, float(train_config['lr']), betas=(float(train_config['beta1']), float(train_config['beta2'])))\n",
    "\n",
    "image_root = '{}/data_train.npy'.format(train_config['train_path'])\n",
    "gt_root = '{}/mask_train.npy'.format(train_config['train_path'])\n",
    "\n",
    "train_loader = get_loader(image_root, gt_root, batchsize=int(train_config['batchsize']))\n",
    "total_step = len(train_loader)\n",
    "\n",
    "print(\"#\"*20, \"Start Training\", \"#\"*20)\n",
    "\n",
    "best_loss = 1e5\n",
    "for epoch in range(1, int(train_config['epoch']) + 1):\n",
    "    best_loss = train(train_loader, model, optimizer, epoch, best_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b8e2b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907e156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config=config['INFERENCE']\n",
    "test_config['ckpt_path']='snapshots/TransFuse_S/TransFuse-7.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982f735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eeda3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model:  snapshots/TransFuse_S/TransFuse-7.pth\n",
      "Dice: 0.8504, IoU: 0.7663, Acc: 0.9392\n"
     ]
    }
   ],
   "source": [
    "model = TransFuse_S().cuda()\n",
    "model.load_state_dict(torch.load(test_config['ckpt_path']))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "if test_config['save_path'] is not None:\n",
    "    os.makedirs(test_config['save_path'], exist_ok=True)\n",
    "\n",
    "print('evaluating model: ', test_config['ckpt_path'])\n",
    "\n",
    "image_root = '{}/data_test.npy'.format(test_config['test_path'])\n",
    "gt_root = '{}/mask_test.npy'.format(test_config['test_path'])\n",
    "test_loader = test_dataset(image_root, gt_root)\n",
    "\n",
    "dice_bank = []\n",
    "iou_bank = []\n",
    "acc_bank = []\n",
    "\n",
    "for i in range(test_loader.size):\n",
    "    image, gt = test_loader.load_data()\n",
    "    img=image[0]\n",
    "    img = torch.transpose(img, 0, 1)\n",
    "    img = torch.transpose(img, 1, 2)\n",
    "    img = img.numpy()\n",
    "    gt_o=gt.copy()\n",
    "    gt = 1*(gt>0.5)\n",
    "    image = image.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, res = model(image)\n",
    "\n",
    "    res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "    res_o=res.copy()\n",
    "    res = 1*(res > 0.5)\n",
    "    \n",
    "\n",
    "    if test_config['save_path'] is not None:\n",
    "        imageio.imwrite(test_config['save_path']+'/'+str(i)+'_img.jpg', img)\n",
    "        imageio.imwrite(test_config['save_path']+'/'+str(i)+'_pred.jpg', res)\n",
    "        imageio.imwrite(test_config['save_path']+'/'+str(i)+'_gt.jpg', gt_o)\n",
    "\n",
    "    dice = mean_dice_np(gt, res)\n",
    "    iou = mean_iou_np(gt, res)\n",
    "    acc = np.sum(res == gt) / (res.shape[0]*res.shape[1])\n",
    "\n",
    "    acc_bank.append(acc)\n",
    "    dice_bank.append(dice)\n",
    "    iou_bank.append(iou)\n",
    "\n",
    "print('Dice: {:.4f}, IoU: {:.4f}, Acc: {:.4f}'.\n",
    "    format(np.mean(dice_bank), np.mean(iou_bank), np.mean(acc_bank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0709a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
